{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trading Physics To Orders Cancels Trades 2.0 -converter\n",
    "##### Usage examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example 1: Download orderbook data from trading physics\n",
    "obdt = OrderBookDataTool()\n",
    "dl_path = 'D:/Data' # target folder\n",
    "ticker = 'SPY' # ticker\n",
    "min_date = '2014-05-26' # first date to get\n",
    "max_date = '2015-01-01' # first date not to get\n",
    "c_num = '111-222-333' # trading physics account customer number\n",
    "pw_hash = 'asd1GHlaR9IU13094u8dLi' # trading physics account password hash\n",
    "obdt.getdata(ticker,min_date,max_date,c_num,pw_hash,dl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example 2: Convert a folder of trading physics imput data into OCT2-format.\n",
    "obdt = OrderBookDataTool()\n",
    "input_folder = 'F:/Data/20130111-SPY.csv'\n",
    "output_folder = 'F:/Output'\n",
    "mode = 'w'\n",
    "obdt.convert(input_folder,output_folder,mode) # convert data to oct2 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example 3: extend initially converted OCT2 data with order book state dependent featuers. \n",
    "ob = OrderBook('SPY','F:/Output/SPY_OCT2.h5')\n",
    "ob.build_features(date='2013_01_11',save_path='F:/Output/SPY_OCT2_PLUS.h5',debug_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example 4: plot ob data using step\n",
    "ob = OrderBook('SPY','C:/temp/Data/output/SPY_OCT2.h5')\n",
    "ylim = [1468000, 1473100]\n",
    "xlim = [-30000, 32000]\n",
    "ob.plot('2013_01_11',67000,ylim,xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example 5: plot ob data using ms time (12:00:00.000)\n",
    "ob = OrderBook('SPY','C:/temp/Data/output/SPY_OCT2.h5')\n",
    "ylim = [1468000, 1473100]\n",
    "xlim = [-30000, 32000]\n",
    "date= '2013_01_11'\n",
    "ob.plot(date,ob.time2step(date,43200000)[0],ylim,xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example 6: plot ob data using time string\n",
    "ob = OrderBook('SPY','C:/temp/Data/output/SPY_OCT2.h5')\n",
    "ylim = [1468000, 1473100]\n",
    "xlim = [-30000, 32000]\n",
    "date= '2013_01_11'\n",
    "strtime = '12:00:00.000'\n",
    "step = ob.str2step(date,strtime)[0]\n",
    "ob.plot(date,step,ylim,xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py as h5          # 2.6.0    + hdf5 1.8.15.1\n",
    "import numpy as np         # 1.11.1\n",
    "import pandas as pd        # 0.19.2\n",
    "import bisect as bs\n",
    "import random as rn\n",
    "import warnings as wrn\n",
    "import requests as rq\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.patches as patches\n",
    "from datetime import datetime as dt\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from IPython import display\n",
    "\n",
    "# figure/plotting specifications\n",
    "%matplotlib inline\n",
    "fig_size = [20,16]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "wrn.filterwarnings('ignore')\n",
    "\n",
    "class OrderBookDataTool(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def convert(self,input_path,output_path,mode):\n",
    "        # params:\n",
    "        # input_path - input file or folder containing (only) input files\n",
    "        # output_path - output folder\n",
    "        # mode - 'w' (over)write or 'a' append\n",
    "        \n",
    "        # ticker = 'SPY' # ticker code\n",
    "        # path_in = 'D:/SPY_project/data_example/input/' # input file location\n",
    "        # path_out = 'D:/SPY_project/data_example/output/' # output file location\n",
    "        # file_out = 'spy_to_hdf_test.h5' # input file name        \n",
    "        \n",
    "        # time the funcintion\n",
    "        start_time = time.time()\n",
    "\n",
    "        # suppress warnings generated by having hdf5 group named with yyyy_mm_dd since it is not really a problem\n",
    "        wrn.filterwarnings('ignore', '.*NaturalNameWarning.*') \n",
    "        \n",
    "        # handle params\n",
    "        if os.path.isfile(input_path): # if input_path points to a single file\n",
    "            file_path ,file_name = os.path.split(input_path)\n",
    "            files = [file_name]\n",
    "        elif os.path.isdir(input_path): # else if input_path points to a directory\n",
    "            files = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))] # get all files in the dir\n",
    "            file_path = input_path\n",
    "        else: # invalid input_path\n",
    "            print('Invalid input_path. Stopping.')\n",
    "            return -2\n",
    "        \n",
    "        ticker = files[0].split('-',2)[1].split('.',2)[0] # get the ticker code from the first file\n",
    "        file_re = re.compile(r'[0-9]{8}-'+ticker+'.csv') # construct file name re\n",
    "        convert_files = (list(filter(file_re.match, files))) # find all that match the re\n",
    "        convert_files = [os.path.join(file_path, f) for f in convert_files] # extend to full paths\n",
    "        if (len(convert_files) != len(files)): # if all files didn't match\n",
    "            print('Invalid output_path. All files should match: \"[0-9]{8}-'+ticker+'.csv\". Stopping.')\n",
    "            return -2\n",
    "        \n",
    "        file_out = ticker + '_OCT2.h5' # output file name  \n",
    "        if os.path.isdir(output_path): # if input_path points to a single file\n",
    "            path_out_full = os.path.join(output_path,file_out)# everything ok\n",
    "        else: # invalid input_path\n",
    "            print('Invalid output_path. Stopping.')\n",
    "            return -3\n",
    "        \n",
    "        if ((mode=='w')|((mode=='a')&(os.path.isfile(path_out_full)))):\n",
    "            pass # everything ok\n",
    "        else: # invalid input_path\n",
    "            print('Invalid mode. Needs to be \"w\"(rite) or \"a\"(ppend). To append the file must exist in output_path. Stopping.')\n",
    "            return -4        \n",
    "        \n",
    "        print('Started converting '+str(len(convert_files))+' file(s).')\n",
    "        \n",
    "        # Loop input files \n",
    "        for file in convert_files:\n",
    "            \n",
    "            loop_time = time.time() # time the loop\n",
    "            \n",
    "            ## get the date from the input file name\n",
    "            path ,file_name = os.path.split(file)\n",
    "            yyyy_str = file_name[0:4] # yyyy date str\n",
    "            mm_str = file_name[4:6] # mm date str\n",
    "            dd_str = file_name[6:8] # dd date str          \n",
    "            date = yyyy_str + mm_str + dd_str # yyyymmdd date str\n",
    "            grp_date = yyyy_str + '_' + mm_str + '_' + dd_str # yyyy_mm_dd date str\n",
    "            \n",
    "            ## Open/Read input file\n",
    "            allcols = {'Time','Ticker','Order','T','Shares','Price', 'MPID', 'X'} # Bring in all data for the testing\n",
    "            cols = {'Time','Order','T','Shares','Price', 'MPID'} # Bring in just the columns that we want\n",
    "            cols_dtype = {'Time': 'u4', 'Ticker':'a3', 'Order':'u4', 'T': 'object', 'Shares': 'i4', 'Price': 'u4', 'MPID': 'object', 'X': 'object'}\n",
    "            df = pd.read_csv(file, dtype = cols_dtype, engine = 'c',usecols = cols) # read input file into pandas datarame\n",
    "            df.reset_index(level=0, inplace=True) # create a column from the index\n",
    "\n",
    "            ## Reorganize the input data into the OCT2 format\n",
    "            ### Create the orders table first\n",
    "            or_df = df.query('T == \"B\" or T == \"S\" or T == \"E\" or T == \"C\"').copy() # filter a dataframe with only new limit order creations\n",
    "            rn_cols = {'index':'STEP','Time':'T_CREATED','Order':'ORDER','T':'SIDE','Shares':'DELTA_Q','Price':'PRICE','MPID':'PARTICIPANT'};\n",
    "            or_df.rename(columns=rn_cols, inplace=True) # rename the columns to match the output format\n",
    "            or_df.loc[:,'STEP'] = or_df['STEP'].astype('u4') # convert the key from int64 to uint32\n",
    "            or_df.loc[or_df['PRICE'] == 0,'DELTA_Q'] = -or_df.loc[or_df['PRICE']==0,'DELTA_Q'] # multiply by -1 the quanities of partials \n",
    "            or_df['UPDATE'] = or_df['SIDE'].copy() # get a copy of the 'SIDE'-column and name it as 'UPDATE'\n",
    "            or_df['STEP_PRIORITY'] = or_df['STEP'].copy() # get a copy of the 'STEP'-column and name it as 'STEP_PRIORITY'\n",
    "            ind_update = (or_df['PRICE'] == 0) # find boolean array indicating rows with order updates\n",
    "            or_df.loc[(ind_update == 0),'UPDATE'] = 'L'; # the non-updates are new (L)imit orders\n",
    "            or_df.loc[ind_update,'STEP_PRIORITY'] = np.nan # the update rows priority is invalid so fill it with nan's\n",
    "            or_df.loc[ind_update,'SIDE'] = np.nan # the update rows data side & price is empty/invalid so fill them with nan's\n",
    "            or_df.loc[ind_update,'PRICE'] = np.nan # the update rows data side & price is empty/invalid so fill them with nan's\n",
    "\n",
    "            ### Filter and use the data on full exections and cancellations to get the destruction times\n",
    "            fd_df = df[['index','Order','Time','T']].query('T == \"F\" or T == \"D\"').copy().set_index('Order') # filter to get full executions or cancellations\n",
    "            orj_df = or_df.join(fd_df,on = ['ORDER'],how = 'left') # join the destruction times back to the order submission dataframe\n",
    "            orj_df.rename(columns={'Time':'T_DESTROYED','index':'S_DESTROYED','T':'DESTROYED_BY'},inplace=True) # rename the newly acquired destruction times accordingly\n",
    "            orj_df.set_index(['ORDER','STEP'],drop=False,inplace=True) # change to a multi-index to group the orders' rows together\n",
    "            orj_df.sort_index(ascending=True, inplace=True) # sort by the new index\n",
    "            ind_update_next = (orj_df.shift(-1)['UPDATE'] != 'L') # find boolean array indicating rows prior to order updates\n",
    "            ind_update_next.iloc[-1] = False # there cannot be an update after the last entry\n",
    "            orj_df.loc[ind_update_next,'S_DESTROYED'] = orj_df.shift(-1).loc[ind_update_next,'STEP'].astype('u4') # assign correct destruction STEPs\n",
    "            orj_df.loc[ind_update_next,'T_DESTROYED'] = orj_df.shift(-1).loc[ind_update_next,'T_CREATED'].astype('u4') # assign correct destruction times\n",
    "            orj_df[['PRICE','SIDE','STEP_PRIORITY']] = orj_df[['PRICE','SIDE','STEP_PRIORITY']].fillna(method='ffill') # forward fill in the price, side and step_priority for the orders updates         \n",
    "            \n",
    "            ### Create the traded column:\n",
    "            ind_cancel_next = (orj_df.shift(-1)['UPDATE'] == 'C') # find boolean array indicating rows prior to partial cancellations\n",
    "            ind_trade_next = (ind_update_next & ~(ind_cancel_next)) # find boolean array indicating rows prior to partial trades   \n",
    "            ind_final_before_full_trade = (~(ind_update_next) & (orj_df['DESTROYED_BY'] == 'F')) # find barray to indicate that there is no more updates and order was eventually traded (F messages)\n",
    "            orj_df['TRADED'] = False # create boolean field to indicate if the order is eventually traded or is eventually cancelled\n",
    "            orj_df.loc[ind_trade_next,'TRADED'] = True # traded bool is true when there is a partial trade next\n",
    "            orj_df.loc[ind_final_before_full_trade,'TRADED'] = True # ... or when there is no more updates and order was eventually traded (F messages)\n",
    "\n",
    "            orj_df.drop(['ORDER','STEP','DESTROYED_BY'],axis=1,inplace=True) # drop the extra columns (they are in the index as well)               \n",
    "            \n",
    "            ### Cumsum to get the remaining quantities. This will take a couple of minutes.\n",
    "            orj_df['QUANTITY'] = orj_df.groupby(level=[0])['DELTA_Q'].transform(pd.Series.cumsum) # cumsum over ORDERs to get quantities            \n",
    "            \n",
    "            ### Cumsum to get the quantities to be traded.\n",
    "            orj_df['DELTA_Q_TO_TRADE'] = 0 # create new temporary column for the following trades delta q's \n",
    "            orj_df.loc[ind_trade_next,'DELTA_Q_TO_TRADE'] = orj_df.shift(-1).loc[ind_trade_next,'DELTA_Q'].astype('i4') # get the partial trade quantities\n",
    "            orj_df.loc[ind_final_before_full_trade,'DELTA_Q_TO_TRADE'] = -orj_df.loc[ind_final_before_full_trade,'QUANTITY'] # get the full trade quantities\n",
    "            # orj_df.index.sortlevel(level=[1],ascending=[False],sort_remaining=False) \n",
    "            orj_df.sort_index(level=[0,1], ascending=[True,False], inplace=True, sort_remaining=False) # sort again to get the decending step order\n",
    "            orj_df['Q_TO_TRADE'] = -orj_df.groupby(level=[0])['DELTA_Q_TO_TRADE'].transform(pd.Series.cumsum) # cumsum to get the remaining  quantities to be traded\n",
    "            orj_df.sort_index(ascending=True,inplace=True) # return to the original order            \n",
    "            \n",
    "            ### Create the cancellations table\n",
    "            cd_df = df[['index','Order', 'Time','T','Shares','MPID']].query('T == \"C\" or T == \"D\"').copy() # filter all of the cancellations\n",
    "            rn_cols = {'index':'STEP','Time':'TIME','Order':'ORDER','T':'UPDATE','Shares':'DELTA_Q','MPID':'PARTICIPANT'};\n",
    "            cd_df.rename(columns=rn_cols, inplace=True) # rename the columns to match the output format\n",
    "            cd_df.set_index(['ORDER','STEP'],inplace =True) # change to a multi-index to group the orders' rows together\n",
    "            cd_df.sort_index(ascending=True, inplace=True) # sort by the new index so that ORDERs are grouped together\n",
    "\n",
    "            ### Join the last updates' remaining quantities back to the cancellation data to determine the delta_q\n",
    "            cdj_df = cd_df.join(orj_df.groupby(level=[0],as_index=True).last(),how = 'left',rsuffix = '_OR')\n",
    "            ixd = (cdj_df['UPDATE'] == 'D') # filter for full cancellations\n",
    "            ixc = (cdj_df['UPDATE'] == 'C') # filter for partial cancellations\n",
    "            cdj_df.loc[ixd,'DELTA_Q'] = -cdj_df.loc[ixd,'QUANTITY'] # update full cancellations to match the remaining quantity times -1\n",
    "            cdj_df.loc[ixc,'DELTA_Q'] = -cdj_df.loc[ixc,'DELTA_Q'] # update partial cancellations to be negative in delta_q\n",
    "            cdj_df.drop(['T_CREATED','DELTA_Q_OR','UPDATE_OR','T_DESTROYED','QUANTITY','S_DESTROYED'], axis=1,inplace=True) # drop the extra columns\n",
    "\n",
    "            ### Create the trades table\n",
    "            ef_df = df[['index','Order', 'Time','T','Price','Shares','MPID']].query('T == \"E\" or T == \"F\" or T == \"T\"').copy() # filter all of the trades\n",
    "            rn_cols = {'index':'STEP','Time':'TIME','Price':'TMP_PRICE','Order':'ORDER','T':'UPDATE','Shares':'DELTA_Q','MPID':'PARTICIPANT'};\n",
    "            ef_df.rename(columns=rn_cols, inplace=True) # rename the columns to match the output format\n",
    "            ef_df.set_index(['ORDER','STEP'],inplace =True) # change to a multi-index to group the orders' rows together\n",
    "            ef_df.sort_index(ascending=True, inplace=True) # sort by the new index so that ORDERs are grouped together\n",
    "            \n",
    "            ### Join the last updates' remaining quantities back to the trade data to determine the delta_q\n",
    "            efj_df = ef_df.join(orj_df.groupby(level=[0],as_index=True).last(),how = 'left',rsuffix = '_OR')\n",
    "            ixf = (efj_df['UPDATE'] == 'F') # filter for full excutions (trades)\n",
    "            ixe = (efj_df['UPDATE'] == 'E') # filter for partial executions\n",
    "            efj_df.loc[ixf,'DELTA_Q'] = -efj_df.loc[ixf,'QUANTITY'] # update full executions to match the remaining quantity times -1\n",
    "            efj_df.loc[~(ixf),'DELTA_Q'] = -efj_df.loc[~(ixf),'DELTA_Q'] # update partial and non-display trades to be negative in delta_q\n",
    "            efj_df.loc[~(ixe|ixf),'PRICE'] = efj_df.loc[~(ixe|ixf),'TMP_PRICE'] # set correct prices for non-display trades\n",
    "            efj_df.drop(['T_CREATED','DELTA_Q_OR','UPDATE_OR','TMP_PRICE','T_DESTROYED','QUANTITY','S_DESTROYED'], \n",
    "                        axis=1,inplace=True) # drop extra columns\n",
    "            efj_df.loc[(efj_df['UPDATE'] == 'T'),['STEP_PRIORITY']] = 0 # fill nans with 0s ...\n",
    "            efj_df['STEP_PRIORITY'] = efj_df['STEP_PRIORITY'].astype('u4') # ... so it's possible to convert them back to unsigned ints\n",
    "            \n",
    "            ### Get the cross events\n",
    "            x_df = df.query('T == \"X\"').copy() # filter a dataframe with only cross events\n",
    "            x_df.drop(['T','MPID'], axis=1,inplace=True)\n",
    "            rn_cols = {'index':'STEP','Time':'TIME','Order':'ORDER','Shares':'QUANTITY','Price':'PRICE'};\n",
    "            x_df.rename(columns=rn_cols, inplace=True) # rename the columns to match the output format\n",
    "            x_df['QUANTITY'] = x_df['QUANTITY'].astype(\"u4\") # dtype to uint32            \n",
    "            \n",
    "            ### Convert datatypes and prepare the data to be written into the hdf5 file\n",
    "            orj_df.reset_index(inplace=True) # Switch back to using row number as the index\n",
    "            orj_df.index.names = ['INDEX'] # Rename with caps so it fits in with the other column names\n",
    "            orj_df['ORDER'] = orj_df['ORDER'].astype(\"u4\") # dtype to uint32\n",
    "            orj_df['T_DESTROYED'] = orj_df['T_DESTROYED'].astype(\"u4\") # dtype to uint32\n",
    "            orj_df['S_DESTROYED'] = orj_df['S_DESTROYED'].astype(\"u4\") # dtype to uint32\n",
    "            orj_df['STEP_PRIORITY'] = orj_df['STEP_PRIORITY'].astype(\"u4\") # dtype to uint32\n",
    "            orj_df['QUANTITY'] = orj_df['QUANTITY'].astype(\"u4\") # dtype to uint32\n",
    "            orj_df['PRICE'] = orj_df['PRICE'].astype(\"u4\") # dtype to uint32\n",
    "            orj_df['DELTA_Q'] = orj_df['DELTA_Q'].astype(\"i4\") # dtype to int32\n",
    "            orj_df['SIDE'] = orj_df['SIDE'].astype('category') # dtype to category\n",
    "            orj_df['UPDATE'] = orj_df['UPDATE'].astype('category') # dtype to category\n",
    "            orj_df['PARTICIPANT'] = orj_df['PARTICIPANT'].astype('category') # dtype to category\n",
    "            rn_cols = {'STEP':'STEP_CREATED','S_DESTROYED':'STEP_DESTROYED','T_DESTROYED':'TIME_DESTROYED','T_CREATED':'TIME_CREATED','DELTA_Q':'DELTA_QUANTITY','Q_TO_TRADE':'QUANTITY_TO_BE_TRADED'}\n",
    "            orj_df.rename(columns=rn_cols, inplace=True) # rename \n",
    "\n",
    "            orj_df['SIDE'].cat.rename_categories(['BID','ASK'],inplace=True) # rename category labels to be more descriptive\n",
    "            orj_df['UPDATE'].cat.rename_categories(['CANCEL','TRADE','ORDER'],inplace=True) # rename category labels to be more descriptive\n",
    "\n",
    "            cdj_df.reset_index(inplace=True) # Switch back to using row number as the index\n",
    "            cdj_df.index.names = ['INDEX'] # Rename with caps so it fits in with the other column names\n",
    "            cdj_df['ORDER'] = cdj_df['ORDER'].astype(\"u4\") # dtype to uint32\n",
    "            cdj_df['STEP'] = cdj_df['STEP'].astype(\"u4\") # dtype to uint32\n",
    "            cdj_df['TIME'] = cdj_df['TIME'].astype(\"u4\") # dtype to uint32\n",
    "            cdj_df['PRICE'] = cdj_df['PRICE'].astype(\"u4\") # dtype to uint32\n",
    "            cdj_df['DELTA_Q'] = cdj_df['DELTA_Q'].astype(\"i4\") # dtype to int32\n",
    "            cdj_df['SIDE'] = cdj_df['SIDE'].astype('category') # dtype to category\n",
    "            cdj_df['UPDATE'] = cdj_df['UPDATE'].astype('category') # dtype to category\n",
    "            cdj_df['PARTICIPANT'] = cdj_df['PARTICIPANT'].astype('category') # dtype to category\n",
    "            rn_cols = {'DELTA_Q':'DELTA_QUANTITY'}\n",
    "            cdj_df.rename(columns=rn_cols, inplace=True) # rename \n",
    "            \n",
    "            cdj_df['SIDE'].cat.rename_categories(['BID','ASK'],inplace=True) # rename category labels to be more descriptive\n",
    "            cdj_df['UPDATE'].cat.rename_categories(['PART','FULL'],inplace=True) # rename category labels to be more descriptive\n",
    "\n",
    "            efj_df.reset_index(inplace=True) # Switch back to using row number as the index\n",
    "            efj_df.index.names = ['INDEX'] # Rename with caps so it fits in with the other column names\n",
    "            efj_df['ORDER'] = efj_df['ORDER'].astype(\"u4\") # dtype to uint32\n",
    "            efj_df['STEP'] = efj_df['STEP'].astype(\"u4\") # dtype to uint32\n",
    "            efj_df['TIME'] = efj_df['TIME'].astype(\"u4\") # dtype to uint32\n",
    "            efj_df['PRICE'] = efj_df['PRICE'].astype(\"u4\") # dtype to uint32\n",
    "            efj_df['DELTA_Q'] = efj_df['DELTA_Q'].astype(\"i4\") # dtype to int32\n",
    "            efj_df['SIDE'] = efj_df['SIDE'].astype('category') # dtype to category\n",
    "            efj_df['UPDATE'] = efj_df['UPDATE'].astype('category') # dtype to category\n",
    "            efj_df['PARTICIPANT'] = efj_df['PARTICIPANT'].astype('category') # dtype to category\n",
    "            rn_cols = {'DELTA_Q':'DELTA_QUANTITY'}\n",
    "            efj_df.rename(columns=rn_cols, inplace=True) # rename \n",
    "\n",
    "            efj_df['SIDE'].cat.rename_categories(['BID','ASK'],inplace=True) # rename category labels to be more descriptive\n",
    "            efj_df['UPDATE'].cat.rename_categories(['PART','FULL','HIDDEN'],inplace=True) # rename category labels to be more descriptive\n",
    "\n",
    "            ## Write the tables into the HDF5 file\n",
    "            cols = ['ORDER','STEP_CREATED','STEP_DESTROYED','STEP_PRIORITY','TIME_CREATED','TIME_DESTROYED','SIDE',\n",
    "                    'UPDATE','PRICE','QUANTITY','DELTA_QUANTITY','TRADED','QUANTITY_TO_BE_TRADED','PARTICIPANT']\n",
    "            grp_orders = ('/' + grp_date + '/ORDERS')\n",
    "            orj_df[cols].to_hdf(path_out_full, grp_orders, mode=mode,format='table',data_columns=cols,complevel=0,complib='blosc') # orders\n",
    "            mode = 'a' # even if mode was write it still needs to be 'a'ppend for the rest of the datasets\n",
    "            cols = ['ORDER','STEP','STEP_PRIORITY','TIME','SIDE','UPDATE','PRICE','DELTA_QUANTITY','PARTICIPANT']\n",
    "            grp_cancels = ('/' + grp_date + '/CANCELS')\n",
    "            cdj_df[cols].to_hdf(path_out_full, grp_cancels, mode=mode,format='table',data_columns=cols,complevel=0,complib='blosc') # cancels\n",
    "            cols = ['ORDER','STEP','STEP_PRIORITY','TIME','SIDE','UPDATE','PRICE','DELTA_QUANTITY','PARTICIPANT']\n",
    "            grp_trades = ('/' + grp_date + '/TRADES')\n",
    "            efj_df[cols].to_hdf(path_out_full, grp_trades, mode=mode,format='table',data_columns=cols,complevel=0,complib='blosc') # trades\n",
    "            cols = ['ORDER','STEP','TIME','PRICE','QUANTITY']\n",
    "            grp_crosses = ('/' + grp_date + '/CROSS_EVENTS')\n",
    "            x_df[cols].to_hdf(path_out_full, grp_crosses, mode=mode,format='table',data_columns=cols,complevel=0,complib='blosc') # cross events\n",
    "            \n",
    "            print('Conversion of '+ file +' complete. It took '+str(time.time()-loop_time)+' seconds.')\n",
    "            \n",
    "        print('Conversion complete. It took '+str(time.time()-start_time)+' seconds.')\n",
    "        return 1\n",
    "\n",
    "    def getdata(self,ticker,min_date,max_date,c_num,pw_hash,dl_path): # Get the data files using trading physics api\n",
    "        start_time = time.time() # time the funcintion       \n",
    "\n",
    "        if os.path.isdir(dl_path): # if dl_path is directory\n",
    "            pass # everything ok\n",
    "        else: # invalid input_path\n",
    "            print('Invalid dl_path. Stopping.')\n",
    "            return -2        \n",
    "        \n",
    "        data_type = 'orderflow' # type of data to get - it should always be 'orderflow' or the converter will not work\n",
    "        data_format = 'CSV'     # format of data to get - it should always be 'CSV' or the converter will not work\n",
    "        data_comp = 'stream'    # compression of data to use - it should always be 'stream' or the converter will not work\n",
    "        api_url = 'http://api.tradingphysics.com' # trading physics data api url\n",
    "\n",
    "        # Get the available dates and filter the ones we want\n",
    "        r = rq.get(api_url+'/getdates',params={'type':data_type})\n",
    "        datelist = r.text.splitlines(); # split by newline\n",
    "        all_dts = np.array([dt.strptime(datestr, '%Y-%m-%d') for datestr in datelist]) # convert strings to datetime objects\n",
    "        min_dt = dt.strptime(min_date, '%Y-%m-%d') # datetime lower limit\n",
    "        max_dt = dt.strptime(max_date, '%Y-%m-%d') # datetime lower limit\n",
    "        get_dts = all_dts[(all_dts < max_dt)&(all_dts >= min_dt)] # find the span of dates to get\n",
    "        get_dts = get_dts[::-1] # flip \n",
    "        \n",
    "        # download data for date each in get_dts\n",
    "        for date in get_dts:\n",
    "\n",
    "            strdate = dt.strftime(date,'%Y%m%d') # make a YYYYmmdd -string from date\n",
    "            filename = strdate+'-'+ticker+'.csv' # create name for the file that will be downloaded\n",
    "\n",
    "            # get the ticket\n",
    "            ticket_params = (('date',strdate),('stock',ticker),('format',data_format),('compression',data_comp))\n",
    "            ticket_url = api_url+'/getticket?C='+c_num+'&P='+pw_hash+'?getdata?type=orderflow' # api doesn't follow normal syntax so this gets messy\n",
    "            r = rq.get(ticket_url,params=ticket_params) # request to get the ticket \n",
    "            if (r.status_code!=200): # if there is a problem with the request\n",
    "                print('Could not get a dl ticket using url: '+r.url)\n",
    "                print('Status code: '+str(r.status_code))\n",
    "                print('Reason: '+r.reason)\n",
    "                print('Stopping because otherwise download credits might be wasted.')\n",
    "                return -1\n",
    "            ticket = r.text # get the ticket from the response\n",
    "            \n",
    "            data_params = (('type',data_type),('date',strdate),('stock',ticker),('format',data_format),\n",
    "                           ('compression',data_comp),('t',ticket))\n",
    "            data_url = api_url+'/getdata' # use the ticket to get the data\n",
    "            r = rq.get(data_url,params=data_params,stream=True) # request to get the data\n",
    "            if (r.status_code==200): # if there is no problem with the request\n",
    "                file = open(os.path.join(dl_path,filename), 'wb')\n",
    "                for chunk in r.iter_content(chunk_size=512 * 1024): # avoid running out of memory with large files -> write in chunks\n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        file.write(chunk)\n",
    "                file.close()   \n",
    "            else:\n",
    "                print('Could not download file using url: '+r.url)\n",
    "                print('Status code: '+str(r.status_code))\n",
    "                print('Reason: '+r.reason)\n",
    "                print('Stopping because otherwise download credits might be wasted.')\n",
    "                return -1\n",
    "        \n",
    "        print('Completed loading of '+str(get_dts.size)+' files.')\n",
    "        print('Time taken: ' +str((time.time()-start_time))+' seconds.')\n",
    "        return 1\n",
    "\n",
    "## Class to build and hold order book states from oct2 data\n",
    "\n",
    "class OrderBookData(object):\n",
    "    # orders, pd.DataFrame    - orders table as a pandas dataframe\n",
    "    # cancels, pd.DataFrame    - cancels table as a pandas dataframe\n",
    "    # trades, pd.DataFrame    - trades table as a pandas dataframe\n",
    "    # cross_trades, pd.DataFrame    - cross_trades table as a pandas dataframe\n",
    "    # tick_size              - int price tick size *10000\n",
    "    \n",
    "    def __init__(self,date,orders,cancels,trades,cross_trades,tick_size):    \n",
    "\n",
    "        self.date = date # date of the data\n",
    "        self.tick_size = tick_size # size of price tick\n",
    "        \n",
    "        # self.fig =  plt.figure()\n",
    "        # self.ax = self.fig.add_subplot(1, 1, 1, facecolor=[0.15,0.15,0.15])  \n",
    "\n",
    "        orders.set_index(['STEP_CREATED','STEP_DESTROYED'],drop=False,inplace=True) # index by timestamps\n",
    "        orders.sort_index(inplace=True) # sort by index so they are in chronological order by creation time (S_CREATED)\n",
    "        \n",
    "        self.bid_orders = orders[(orders['SIDE'] == 'BID')].copy() # filter to get only bid side orders\n",
    "        self.ask_orders = orders[(orders['SIDE'] == 'ASK')].copy() # filter to get only bid side orders \n",
    "        \n",
    "        self.bsc = self.bid_orders.ix[:,'STEP_CREATED'] # get list of bid state creation times\n",
    "        self.bsd = self.bid_orders.ix[:,'STEP_DESTROYED'] # get list of bid state destruction times\n",
    "        self.ssc = self.ask_orders.ix[:,'STEP_CREATED'] # get list of bid state creation times\n",
    "        self.ssd = self.ask_orders.ix[:,'STEP_DESTROYED'] # get list of bid state destruction times        \n",
    "\n",
    "        self.orders = orders\n",
    "        self.cancels = cancels.set_index(['STEP'],drop=False)\n",
    "        self.trades = trades.set_index(['STEP'],drop=False) \n",
    "        self.cross_trades = cross_trades.set_index(['STEP'],drop=False)\n",
    "        self.nd_trades = self.trades[self.trades['ORDER']==0]\n",
    "        \n",
    "        # create events dataframe\n",
    "        events = orders[['STEP_CREATED','STEP_PRIORITY','TIME_CREATED','SIDE','PRICE','UPDATE']]\n",
    "        rncols = {'STEP_CREATED':'STEP','TIME_CREATED':'TIME','UPDATE':'UPDATE_OLD'}\n",
    "        events.rename(columns=rncols,inplace=True)\n",
    "        events.set_index(['STEP'],drop=False,inplace=True) \n",
    "        events.loc[events['UPDATE_OLD']=='ORDER','UPDATE'] = 'O'\n",
    "        events.loc[events['UPDATE_OLD']=='TRADE','UPDATE'] = 'PT'\n",
    "        events.loc[events['UPDATE_OLD']=='CANCEL','UPDATE'] = 'PC'\n",
    "        \n",
    "        cancels = cancels[cancels['UPDATE']=='FULL']\n",
    "        cancels = cancels[['STEP','STEP_PRIORITY','TIME','SIDE','PRICE']]\n",
    "        cancels.set_index(['STEP'],drop=False,inplace=True) \n",
    "        cancels['UPDATE'] = 'C'\n",
    "        events = events.append(cancels)\n",
    "        \n",
    "        trades = trades[trades['UPDATE']=='FULL']\n",
    "        trades = trades[['STEP','STEP_PRIORITY','TIME','SIDE','PRICE']]\n",
    "        trades.set_index(['STEP'],drop=False,inplace=True) \n",
    "        trades['UPDATE'] = 'T'\n",
    "        events = events.append(trades)\n",
    "        \n",
    "        events['UPDATE'] = events['UPDATE'].astype('category')\n",
    "        events.drop('UPDATE_OLD',axis=1,inplace=True)\n",
    "        events['UPDATE'].cat.rename_categories(['CANCEL','ORDER','PART_CANCEL','PART_TRADE','TRADE'],inplace=True)  \n",
    "        events.sort_index(ascending=True, inplace=True)\n",
    "        self.events = events\n",
    "        self.ask_events = events[events['SIDE']=='ASK'].copy()\n",
    "        self.bid_events = events[events['SIDE']=='BID'].copy()\n",
    "\n",
    "    def update_rm_ask_state(self,step): # update ask side by removing everything that has expired up to current step\n",
    "        self.ask_state = self.ask_state[self.ask_state['STEP_DESTROYED']!=step]   \n",
    "    \n",
    "    def update_rm_bid_state(self,step): # update bid side by removing everything that has expired up to current step\n",
    "        self.bid_state = self.bid_state[self.bid_state['STEP_DESTROYED']!=step]\n",
    "\n",
    "    def update_add_ask_state(self,step):  # update ask side by adding current step order\n",
    "        self.ask_state = pd.concat([self.ask_state, self.ask_orders.xs(step, level=0, drop_level=False)])\n",
    "    \n",
    "    def update_add_bid_state(self,step): # update bid side by adding current step order\n",
    "        self.bid_state = pd.concat([self.bid_state, self.bid_orders.xs(step, level=0, drop_level=False)])\n",
    "    \n",
    "    def update_addrm_ask_state(self,step): # update bid side by removing everything that has expired up to current step\n",
    "        self.ask_state = pd.concat([self.ask_state[self.ask_state['STEP_DESTROYED']!=step], self.ask_orders.xs(step, level=0, drop_level=False)])\n",
    "     \n",
    "    def update_addrm_bid_state(self,step): # update bid side by adding current step order\n",
    "        self.bid_state = pd.concat([self.bid_state[self.bid_state['STEP_DESTROYED']!=step], self.bid_orders.xs(step, level=0, drop_level=False)])    \n",
    "    \n",
    "    def init_ask_state(self,step): # get the collection of orders on ask side (regardless of sequence of states)\n",
    "        self.ask_state = self.ask_orders[(self.ssc <= step)&(self.ssd > step)]\n",
    "        pass\n",
    "        \n",
    "    def init_bid_state(self,step): # get the collection of orders on bid side (regardless of sequence of states)\n",
    "        self.bid_state = self.bid_orders[(self.bsc <= step)&(self.bsd > step)]\n",
    "        pass\n",
    "    \n",
    "    def update_event_ask_lvl(self,p): # get the level relative to ask side events price: 0.5 if p is a new best, 1 at lvl1, 1.5 if at [lvl1-lvl2] etc...\n",
    "        if not self.ask_levels.empty:                           \n",
    "            lvl = bs.bisect_left(self.ask_levels.index.values,p) + 1 # ask level of the event\n",
    "            return (lvl if (p in self.ask_levels.index.values) else lvl - 0.5)\n",
    "        else:\n",
    "            return 0\n",
    "                                                                          \n",
    "    def update_event_bid_lvl(self,p):  # get the level relative to bid side events price: 0.5 if p is a new best, 1 at lvl1, 1.5 if at [lvl1-lvl2] etc...\n",
    "        if not self.bid_levels.empty: \n",
    "            lvl = bs.bisect_left(-self.bid_levels.index.values[::-1],-p) + 1 # bid level of the event\n",
    "            return (lvl if (p in self.bid_levels.index.values) else lvl - 0.5)\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    def update_ask_tick(self,p): # get the ask side updates tick depth of ask order (deeper means lower price priority negatives are inside the spread)\n",
    "        return (p-self.s_best)/self.tick_size if not(np.isnan(self.s_best)) else np.nan\n",
    "    \n",
    "    def update_bid_tick(self,p): # get the bid side updates tick depth of ask order (deeper means lower price priority negatives are inside the spread)\n",
    "        return (self.b_best-p)/self.tick_size if not(np.isnan(self.b_best)) else np.nan\n",
    "    \n",
    "    def update_nth_ask_lvl(self,n): # update ask levels and return nth level [p,q]\n",
    "        self.ask_levels = self.ask_state[['PRICE','QUANTITY']].groupby(['PRICE']).agg({'QUANTITY': np.sum}).sort_index()\n",
    "        if not self.ask_levels.empty:\n",
    "            self.s_best = self.ask_levels.index[n-1]\n",
    "            return [self.ask_levels.index[n-1], self.ask_levels['QUANTITY'].iloc[n-1]]\n",
    "        else: \n",
    "            return [-1,-1]\n",
    "    \n",
    "    def update_nth_bid_lvl(self,n): # update bid levels and return nth level [p,q]\n",
    "        self.bid_levels = self.bid_state[['PRICE','QUANTITY']].groupby(['PRICE']).agg({'QUANTITY': np.sum}).sort_index()\n",
    "        if not self.bid_levels.empty:\n",
    "            self.b_best = self.bid_levels.index[-n]\n",
    "            return [self.bid_levels.index[-n], self.bid_levels['QUANTITY'].iloc[-n]]\n",
    "        else:\n",
    "            return [-1,-1]\n",
    "    \n",
    "    def update_ask_q_in_front(self,p,sp): # cumulative quantity of orders with higher priority on ask side\n",
    "        orders_in_front = ((self.ask_state['PRICE']<p) | ((self.ask_state['PRICE'] == p) & (self.ask_state['STEP_PRIORITY'] < sp))) \n",
    "        if any(orders_in_front):\n",
    "            return self.ask_state.loc[orders_in_front,'QUANTITY'].sum()\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def update_bid_q_in_front(self,p,sp): # cumulative quantity of orders with higher priority on bid side\n",
    "        orders_in_front = ((self.bid_state['PRICE']>p) | ((self.bid_state['PRICE'] == p) & (self.bid_state['STEP_PRIORITY'] < sp))) \n",
    "        if any(orders_in_front):\n",
    "            return self.bid_state.loc[orders_in_front,'QUANTITY'].sum()\n",
    "        else:\n",
    "            return 0       \n",
    "        \n",
    "    def init_first_state(self): # start the order book at the first (empty) state \n",
    "        self.init_bid_state(-1)\n",
    "        self.init_ask_state(-1)\n",
    "        self.update_nth_ask_lvl(1)\n",
    "        self.update_nth_bid_lvl(1)\n",
    "        self.b_best = np.nan\n",
    "        self.s_best = np.nan\n",
    "         \n",
    "    def update_ask_data_on_order(self,row): # calculate and return various features that depend on the order book state and the ask side event\n",
    "        al = self.update_event_ask_lvl(row['PRICE'])\n",
    "        at = self.update_ask_tick(row['PRICE']) if (al > 0) else np.nan   # event & old state relative\n",
    "        self.update_add_ask_state(row['STEP']) # update state with event  \n",
    "        aqif = self.update_ask_q_in_front(row['PRICE'],row['STEP_PRIORITY'])\n",
    "        [ap,aq] = self.update_nth_ask_lvl(1) # 1st level price and quantity at the new state\n",
    "        return pd.Series([ap ,aq , al, at, aqif])     \n",
    "        \n",
    "    def update_bid_data_on_order(self,row): # calculate and return various features that depend on the order book state and the bid side event\n",
    "        bl = self.update_event_bid_lvl(row['PRICE']) # event & old state relative\n",
    "        bt = self.update_bid_tick(row['PRICE']) if (bl > 0) else np.nan # event & old state relative       \n",
    "        self.update_add_bid_state(row['STEP']) # update state with event  \n",
    "        bqif = self.update_bid_q_in_front(row['PRICE'],row['STEP_PRIORITY'])\n",
    "        [bp,bq] = self.update_nth_bid_lvl(1) # 1st level price and quantity at the new state\n",
    "        return pd.Series([bp ,bq , bl, bt, bqif]) \n",
    "\n",
    "    def update_ask_data_on_cancel(self,row): # calculate and return various features that depend on the order book state and the ask side event\n",
    "        al = self.update_event_ask_lvl(row['PRICE'])\n",
    "        at = self.update_ask_tick(row['PRICE']) if (al > 0) else np.nan # event & old state relative   \n",
    "        self.update_rm_ask_state(row['STEP']) # update state with event\n",
    "        aqif = self.update_ask_q_in_front(row['PRICE'],row['STEP_PRIORITY'])\n",
    "        [ap,aq] = self.update_nth_ask_lvl(1) # 1st level price and quantity at the new state\n",
    "        return pd.Series([ap ,aq , al, at, aqif])     \n",
    " \n",
    "    def update_bid_data_on_cancel(self,row): # calculate and return various features that depend on the order book state and the bid side event\n",
    "        bl = self.update_event_bid_lvl(row['PRICE']) # event & old state relative\n",
    "        bt = self.update_bid_tick(row['PRICE']) if (bl > 0) else np.nan# event & old state relative\n",
    "        self.update_rm_bid_state(row['STEP']) # update state with event  \n",
    "        bqif = self.update_bid_q_in_front(row['PRICE'],row['STEP_PRIORITY'])\n",
    "        [bp,bq] = self.update_nth_bid_lvl(1) # 1st level price and quantity at the new state\n",
    "        return pd.Series([bp ,bq , bl, bt, bqif]) \n",
    " \n",
    "    def update_ask_data_on_part_cancel(self,row): # calculate and return various features that depend on the order book state and the ask side event\n",
    "        al = self.update_event_ask_lvl(row['PRICE'])\n",
    "        at = self.update_ask_tick(row['PRICE']) if (al > 0) else np.nan # event & old state relative        \n",
    "        self.update_addrm_ask_state(row['STEP']) # update state with event\n",
    "        aqif = self.update_ask_q_in_front(row['PRICE'],row['STEP_PRIORITY'])\n",
    "        [ap,aq] = self.update_nth_ask_lvl(1) # 1st level price and quantity at the new state\n",
    "        return pd.Series([ap ,aq , al, at, aqif])     \n",
    " \n",
    "    def update_bid_data_on_part_cancel(self,row): # calculate and return various features that depend on the order book state and the bid side event\n",
    "        bl = self.update_event_bid_lvl(row['PRICE']) # event & old state relative\n",
    "        bt = self.update_bid_tick(row['PRICE']) if (bl > 0) else np.nan# event & old state relative\n",
    "        self.update_addrm_bid_state(row['STEP']) # update state with event\n",
    "        bqif = self.update_bid_q_in_front(row['PRICE'],row['STEP_PRIORITY'])\n",
    "        [bp,bq] = self.update_nth_bid_lvl(1) # 1st level price and quantity at the new state\n",
    "        return pd.Series([bp ,bq , bl, bt, bqif]) \n",
    "\n",
    "    def update_ask_data_on_trade(self,row): # process ask side trades\n",
    "        self.update_rm_ask_state(row['STEP']) # update state with event\n",
    "        [ap,aq] = self.update_nth_ask_lvl(1) # 1st level price and quantity at the new state        \n",
    "        return pd.Series([ap ,aq, 1.0, 0.0, 0.0]) \n",
    "      \n",
    "    def update_bid_data_on_trade(self,row): # process bid side trades\n",
    "        self.update_rm_bid_state(row['STEP']) # update state with event\n",
    "        [bp,bq] = self.update_nth_bid_lvl(1) # 1st level price and quantity at the new state\n",
    "        return pd.Series([bp ,bq, 1.0, 0.0, 0.0])    \n",
    "  \n",
    "    def update_ask_data_on_part_trade(self,row): # process ask side trades\n",
    "        self.update_addrm_ask_state(row['STEP']) # update state with event\n",
    "        [ap,aq] = self.update_nth_ask_lvl(1) # 1st level price and quantity at the new state        \n",
    "        return pd.Series([ap ,aq, 1.0, 0.0, 0.0]) \n",
    "      \n",
    "    def update_bid_data_on_part_trade(self,row): # process bid side trades\n",
    "        self.update_addrm_bid_state(row['STEP']) # update state with event\n",
    "        [bp,bq] = self.update_nth_bid_lvl(1) # 1st level price and quantity at the new state\n",
    "        return pd.Series([bp ,bq ,1.0, 0.0, 0.0])    \n",
    "\n",
    "    def update_data_on_nd_trade(self,row): # process non display trades (very innefficient but there are only very few of them)\n",
    "        self.init_bid_state(row['STEP']) # update state with event \n",
    "        [bp,bq] = self.update_nth_bid_lvl(1) # 1st level price and quantity at the new state\n",
    "        self.init_ask_state(row['STEP']) # update state with event  \n",
    "        [ap,aq] = self.update_nth_ask_lvl(1) # 1st level price and quantity at the new state \n",
    "        mt = ( row['PRICE'] - ((ap-bp)/2.0) )/self.tick_size\n",
    "        return pd.Series([ap, aq, bp, bq, mt])                                         \n",
    "\n",
    "    def init_updates(self): # unit function dictionaries for different updates \n",
    "        self.ask_updates = {\n",
    "            'ORDER' : self.update_ask_data_on_order,\n",
    "            'CANCEL' : self.update_ask_data_on_cancel,\n",
    "            'PART_CANCEL' : self.update_ask_data_on_part_cancel,\n",
    "            'TRADE' : self.update_ask_data_on_trade,\n",
    "            'PART_TRADE' : self.update_ask_data_on_part_trade\n",
    "        }        \n",
    "        self.bid_updates = {\n",
    "            'ORDER' : self.update_bid_data_on_order,\n",
    "            'CANCEL' : self.update_bid_data_on_cancel,\n",
    "            'PART_CANCEL' : self.update_bid_data_on_part_cancel,\n",
    "            'TRADE' : self.update_bid_data_on_trade,\n",
    "            'PART_TRADE' : self.update_bid_data_on_part_trade\n",
    "        }\n",
    "        \n",
    "    def update_ask_data(self,row): # update ask side data after ask side event\n",
    "        return self.ask_updates[row['UPDATE']](row)\n",
    "        \n",
    "    def update_bid_data(self,row): # update bid side data after bid side event\n",
    "        return self.bid_updates[row['UPDATE']](row)        \n",
    "    \n",
    "    def build_ask_features(self): # build all of the ask side dependent features\n",
    "        self.init_first_state()\n",
    "        self.ask_events[['ASK_PRICE','ASK_QUANTITY','UPDATE_LEVEL','TICK_DIST','QUANTITY_IN_FRONT']] = self.ask_events.apply(\n",
    "            lambda row: self.update_ask_data(row),axis=1)                   \n",
    "        return\n",
    "        \n",
    "    def build_bid_features(self): # build all of the bid side dependent features\n",
    "        self.init_first_state()\n",
    "        self.bid_events[['BID_PRICE','BID_QUANTITY','UPDATE_LEVEL','TICK_DIST','QUANTITY_IN_FRONT']] = self.bid_events.apply(\n",
    "            lambda row: self.update_bid_data(row),axis=1) \n",
    "        return\n",
    "                                       \n",
    "    def build_nd_features(self): # build the non-display trades relatad features\n",
    "        self.nd_trades[['ASK_PRICE','ASK_QUANTITY','BID_PRICE','BID_QUANTITY','TICK_DIST']] = self.nd_trades.apply(\n",
    "            lambda row: self.update_data_on_nd_trade(row),axis=1)\n",
    "        return\n",
    "    \n",
    "    def join_event_data(self): # fill the blanks and prepare orders, cancels and trades tables\n",
    "        self.events = self.bid_events.append(self.ask_events)\n",
    "        \n",
    "        self.nd_trades = self.nd_trades[['STEP','STEP_PRIORITY','TIME','SIDE','PRICE','ASK_PRICE','ASK_QUANTITY','BID_PRICE','BID_QUANTITY','TICK_DIST']]\n",
    "        self.nd_trades['UPDATE_LEVEL'] = np.nan\n",
    "        self.nd_trades['TICK_DIST'] = np.nan\n",
    "        self.nd_trades.set_index(['STEP'],drop=False,inplace=True) \n",
    "        \n",
    "        self.events = self.events.append(self.nd_trades)\n",
    "        self.events.sort_index(ascending=True,inplace=True)\n",
    "        \n",
    "        # fill nan values\n",
    "        self.events['ASK_PRICE'].fillna(method='ffill',inplace=True)\n",
    "        self.events['ASK_QUANTITY'].fillna(method='ffill',inplace=True)\n",
    "        self.events['BID_PRICE'].fillna(method='ffill',inplace=True)\n",
    "        self.events['BID_QUANTITY'].fillna(method='ffill',inplace=True)\n",
    "        self.events['ASK_PRICE'].fillna(-1,inplace=True)\n",
    "        self.events['ASK_QUANTITY'].fillna(-1,inplace=True)\n",
    "        self.events['BID_PRICE'].fillna(-1,inplace=True)\n",
    "        self.events['BID_QUANTITY'].fillna(-1,inplace=True)   \n",
    "        \n",
    "        # convert datatypes to int32\n",
    "        self.events['ASK_PRICE'] = self.events['ASK_PRICE'].astype('i4')\n",
    "        self.events['ASK_QUANTITY'] = self.events['ASK_QUANTITY'].astype('i4')\n",
    "        self.events['BID_PRICE'] = self.events['BID_PRICE'].astype('i4')\n",
    "        self.events['BID_QUANTITY'] = self.events['BID_QUANTITY'].astype('i4')\n",
    "        \n",
    "        self.orders.set_index('STEP_CREATED',inplace=True,drop=False)\n",
    "        self.orders.sort_index()\n",
    "        self.cancels.set_index('STEP',inplace=True,drop=False)\n",
    "        self.cancels.sort_index()\n",
    "        self.trades.set_index('STEP',inplace=True,drop=False)\n",
    "        self.trades.sort_index()\n",
    "        \n",
    "        self.orders = self.orders.join(self.events[['ASK_PRICE','ASK_QUANTITY','BID_PRICE','BID_QUANTITY','UPDATE_LEVEL','TICK_DIST','QUANTITY_IN_FRONT']],how='left')\n",
    "        self.cancels = self.cancels.join(self.events[['ASK_PRICE','ASK_QUANTITY','BID_PRICE','BID_QUANTITY','UPDATE_LEVEL','TICK_DIST','QUANTITY_IN_FRONT']],how='left')\n",
    "        self.trades = self.trades.join(self.events[['ASK_PRICE','ASK_QUANTITY','BID_PRICE','BID_QUANTITY','UPDATE_LEVEL','TICK_DIST','QUANTITY_IN_FRONT']],how='left')\n",
    "        \n",
    "    def build_features(self,debug_mode): # build features    \n",
    "        start = time.time()\n",
    "        print('Building features for ' + self.date + '.')\n",
    "        \n",
    "        if (debug_mode==True):\n",
    "            lim = 5000  \n",
    "            blim = (self.bid_events['STEP'] < lim) # limit for testing and and debugging\n",
    "            alim = (self.ask_events['STEP'] < lim) # limit for testing and and debugging\n",
    "            self.bid_events = self.bid_events[(blim)] # limit for testing and and debugging\n",
    "            self.ask_events = self.ask_events[(alim)] # limit for testing and and debugging      \n",
    "        \n",
    "        self.init_updates()\n",
    "        self.build_ask_features()\n",
    "        self.build_bid_features()\n",
    "        self.build_nd_features()\n",
    "        self.join_event_data()\n",
    "        \n",
    "        print('Building features done. The process took ' +str((time.time()-start)/60)+' minutes.')\n",
    "        \n",
    "    def save_data(self,save_path):\n",
    "    \n",
    "        print('Saving extended data for ' + self.date + '.')\n",
    "    \n",
    "        # categorize the text columns\n",
    "        self.orders['UPDATE'] = self.orders['UPDATE'].astype('category')\n",
    "        self.orders['SIDE'] = self.orders['SIDE'].astype('category')\n",
    "        self.orders['PARTICIPANT'] = self.orders['PARTICIPANT'].astype('category')\n",
    " \n",
    "        self.cancels['UPDATE'] = self.cancels['UPDATE'].astype('category')\n",
    "        self.cancels['SIDE'] = self.cancels['SIDE'].astype('category')\n",
    "        self.cancels['PARTICIPANT'] = self.cancels['PARTICIPANT'].astype('category')\n",
    "\n",
    "        self.trades['UPDATE'] = self.trades['UPDATE'].astype('category')\n",
    "        self.trades['SIDE'] = self.trades['SIDE'].astype('category')\n",
    "        self.trades['PARTICIPANT'] = self.trades['PARTICIPANT'].astype('category')\n",
    "        \n",
    "        ## Write the tables into the HDF5 file\n",
    "        mode = 'w'\n",
    "        cols = ['ORDER','STEP_CREATED','STEP_DESTROYED','STEP_PRIORITY','TIME_CREATED','TIME_DESTROYED','SIDE','UPDATE','PRICE','QUANTITY','DELTA_QUANTITY','TRADED','QUANTITY_TO_BE_TRADED','PARTICIPANT','ASK_PRICE','ASK_QUANTITY','BID_PRICE','BID_QUANTITY','UPDATE_LEVEL','TICK_DIST','QUANTITY_IN_FRONT']\n",
    "        grp_orders = ('/' + self.date + '/ORDERS')\n",
    "        self.orders[cols].to_hdf(save_path, grp_orders, mode=mode,format='table',data_columns=cols,complevel=0,complib='blosc') # orders\n",
    "        mode = 'a' # even if mode was write it still needs to be 'a'ppend for the rest of the datasets\n",
    "        cols = ['ORDER','STEP','STEP_PRIORITY','TIME','SIDE','UPDATE','PRICE','DELTA_QUANTITY','PARTICIPANT','ASK_PRICE','ASK_QUANTITY','BID_PRICE','BID_QUANTITY','UPDATE_LEVEL','TICK_DIST','QUANTITY_IN_FRONT']\n",
    "        grp_cancels = ('/' + self.date + '/CANCELS')\n",
    "        self.cancels[cols].to_hdf(save_path, grp_cancels, mode=mode,format='table',data_columns=cols,complevel=0,complib='blosc') # cancels\n",
    "        cols = ['ORDER','STEP','STEP_PRIORITY','TIME','SIDE','UPDATE','PRICE','DELTA_QUANTITY','PARTICIPANT','ASK_PRICE','ASK_QUANTITY','BID_PRICE','BID_QUANTITY','UPDATE_LEVEL','TICK_DIST','QUANTITY_IN_FRONT']\n",
    "        grp_trades = ('/' + self.date + '/TRADES')\n",
    "        self.trades[cols].to_hdf(save_path, grp_trades, mode=mode,format='table',data_columns=cols,complevel=0,complib='blosc') # trades\n",
    "        cols = ['ORDER','STEP','TIME','PRICE','QUANTITY']\n",
    "        grp_crosses = ('/' + self.date + '/CROSS_EVENTS')\n",
    "        self.cross_trades[cols].to_hdf(save_path, grp_crosses, mode=mode,format='table',data_columns=cols,complevel=0,complib='blosc') # cross events\n",
    "        print('Extended data for ' + self.date +' saved to ' +save_path+'.')\n",
    "        \n",
    "    def time2step(self,t): # convert time (ms from start of day) to the a pair [first step / last step]\n",
    "        ss = self.events.loc[self.events['TIME']==t,'STEP']\n",
    "        if(not ss.empty):\n",
    "            return [min(ss),max(ss)]\n",
    "        else:\n",
    "            ss = self.events.loc[self.events['TIME']<t,'STEP']\n",
    "            return [ss.iloc[-1],ss.iloc[-1]]\n",
    "    \n",
    "    def step2time(self,s): # convert step to time (ms from start of day)\n",
    "        return self.events.loc[s,'TIME']\n",
    "        \n",
    "    def time2str(self,t): # convert time (ms from start of day) to string ('HH:MM:SS.mmm') representing that time\n",
    "        hrs = math.floor(t / (60*60*1000*1.0))\n",
    "        mins = math.floor((t - hrs*60*60*1000) / (60*1000*1.0))\n",
    "        secs = math.floor((t - hrs*60*60*1000 - mins*60*1000) / (1000*1.0))\n",
    "        mils = (t - hrs*60*60*1000 - mins*60*1000 - secs*1000)\n",
    "        return time.strftime(\"%H:%M:%S.{}\".format(mils), (1970,1,1,hrs,mins,secs,0,0,0) )\n",
    "    \n",
    "    def str2time(self,s):\n",
    "        ms = int(s[-3:])\n",
    "        t = time.strptime(s[:-4],\"%H:%M:%S\")\n",
    "        return t[3]*60*60*1000+t[4]*60*1000+t[5]*1000+ms # time in milliseconds since midnight\n",
    "\n",
    "    def plot(self,step,ylim,xlim):      \n",
    "        \n",
    "        print(self.time2str(self.step2time(step)))                \n",
    "                        \n",
    "        tick = self.tick_size\n",
    "        border = 30\n",
    "        alp = 0.5\n",
    "\n",
    "        self.ax.clear()\n",
    "        self.init_ask_state(step)\n",
    "        self.init_bid_state(step)\n",
    "        \n",
    "        ss = self.ask_state.set_index(['PRICE','STEP_PRIORITY','ORDER'],drop=False)\n",
    "        ss.sort_index(inplace=True)\n",
    "        bs = self.bid_state.set_index(['PRICE','STEP_PRIORITY','ORDER'],drop=False)\n",
    "        bs.sort_index(inplace=True)        \n",
    "        \n",
    "        maxq = 0\n",
    "        minq = 0\n",
    "        lastp = 0\n",
    "        lastq = 0\n",
    "        bflip = False\n",
    "        \n",
    "        for ind, row in ss.iterrows():\n",
    "            if (row['PRICE']!=lastp):\n",
    "                maxq = max(lastq,maxq)\n",
    "                lastp = 0\n",
    "                lastq = 0\n",
    "                bflip = False               \n",
    "            col_ca = 'sandybrown' if bflip==True else 'orangered'\n",
    "            col_tr = 'cyan' if bflip==True else 'darkcyan' \n",
    "\n",
    "            self.ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (lastq, row['PRICE']-(tick-border)/2),   # (x,y)\n",
    "                    row['QUANTITY'],                         # width\n",
    "                    (tick-border),                           # height\n",
    "                    facecolor=col_ca,\n",
    "                    alpha=alp  \n",
    "                    )               \n",
    "                )\n",
    "            \n",
    "            self.ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (lastq, row['PRICE']-(tick-border)/2),    # (x,y)\n",
    "                    row['QUANTITY_TO_TRADE'],                 # width\n",
    "                    (tick-border),                            # height\n",
    "                    facecolor=col_tr,\n",
    "                    alpha=alp  \n",
    "                    )\n",
    "                )            \n",
    "            \n",
    "            lastp = row['PRICE']\n",
    "            lastq += row['QUANTITY']\n",
    "            bflip = not bflip\n",
    "\n",
    "        lastp = 0\n",
    "        lastq = 0\n",
    "        bflip = False            \n",
    "            \n",
    "        for ind, row in bs.iterrows():\n",
    "            if (row['PRICE']!=lastp):\n",
    "                \n",
    "                minq = min(-lastq,minq)\n",
    "                lastp = 0\n",
    "                lastq = 0\n",
    "                bflip = False               \n",
    "            \n",
    "            col_ca = 'palegreen' if bflip==True else 'seagreen'\n",
    "            col_tr = 'magenta' if bflip==True else 'darkmagenta' \n",
    "            \n",
    "            self.ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (-row['QUANTITY']-lastq, row['PRICE']-(tick-border)/2),   # (x,y)\n",
    "                    row['QUANTITY'],                                          # width\n",
    "                    (tick-border),                                            # height\n",
    "                    facecolor=col_ca,\n",
    "                    alpha=alp  \n",
    "                    )               \n",
    "                )\n",
    "            \n",
    "            self.ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (-row['QUANTITY_TO_TRADE']-lastq, row['PRICE']-(tick-border)/2),   # (x,y)\n",
    "                    row['QUANTITY_TO_TRADE'],                                          # width\n",
    "                    (tick-border),                                                     # height\n",
    "                    facecolor=col_tr,\n",
    "                    alpha=alp  \n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            lastp = row['PRICE']\n",
    "            lastq += row['QUANTITY']\n",
    "            bflip = not bflip\n",
    "                         \n",
    "        \n",
    "        self.ax.set_ylim(ylim[0],ylim[1])\n",
    "        self.ax.set_xlim(xlim[0],xlim[1])\n",
    "        display.display(self.fig)\n",
    "        return 1\n",
    "\n",
    "class OrderBook(object):\n",
    "    \n",
    "    def __init__(self,ticker,data_path):\n",
    "        print('Started order book ' + ticker)\n",
    "        self.ticker = ticker\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        self.data = {}\n",
    "        self.file = h5.File(data_path,'r')\n",
    "        self.dates = [date for date in self.file]\n",
    "        self.file.close()\n",
    "        \n",
    "        for date in self.dates:\n",
    "            print('Reading in data for '+ date)\n",
    "            orders = pd.read_hdf(data_path,date+'/ORDERS')\n",
    "            cancels = pd.read_hdf(data_path,date+'/CANCELS')\n",
    "            trades = pd.read_hdf(data_path,date+'/TRADES')\n",
    "            cross_trades = pd.read_hdf(data_path,date+'/CROSS_EVENTS')\n",
    "            self.data[date] = OrderBookData(date,orders,cancels,trades,cross_trades,100)\n",
    "            \n",
    "    def build_features(self,date,save_path,debug_mode=False):\n",
    "        self.data[date].build_features(debug_mode)\n",
    "        self.data[date].save_data(save_path)\n",
    "        pass\n",
    "    \n",
    "    def plot(self,date,step,ylim,xlim):\n",
    "        self.data[date].plot(step,ylim,xlim)\n",
    "        \n",
    "    def time2step(self,date,time):\n",
    "        return self.data[date].time2step(time)\n",
    "        \n",
    "    def step2time(self,date,step):\n",
    "        return self.date[date].step2time(step)\n",
    "    \n",
    "    def time2str(self,date,time):\n",
    "        return self.data[date].time2str(time)\n",
    "    \n",
    "    def str2step(self,date,strtime):\n",
    "        return self.data[date].time2step(self.data[date].str2time(strtime))\n",
    "                                                                          \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
